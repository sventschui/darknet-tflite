{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcb4a97",
   "metadata": {},
   "source": [
    "# Darknet ONNX to tflite conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015c5bb",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnx2tf tensorflow tf-keras onnx onnx-graphsurgeon ai-edge-litert sng4onnx onnxsim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c4823",
   "metadata": {},
   "source": [
    "## Generate int8 quantization calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "import functions.parse_darknet_cfg as parse_darknet_cfg_mod\n",
    "\n",
    "DARKNET_CONFIG_PATH = \"people-r-people_2025-05-05/people-r-people.cfg\"\n",
    "IMAGES_PATH = \"people-r-people_2025-05-05/sample_images\"\n",
    "\n",
    "net, layers = parse_darknet_cfg_mod.parse_darknet_cfg(DARKNET_CONFIG_PATH)\n",
    "\n",
    "files = glob.glob(f\"{IMAGES_PATH}/*.jpg\")\n",
    "images = []\n",
    "for idx, file in enumerate(files):\n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.resize(img, dsize=(net[\"height\"], net[\"width\"]))\n",
    "    img = img.astype(np.float32) / 255.0  # convert to 0.0 - 1.0 scale\n",
    "    images.append(img)\n",
    "\n",
    "np.save(file=f\"{IMAGES_PATH}/calibdata.npy\", arr=np.stack(images, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b59cf",
   "metadata": {},
   "source": [
    "## Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2bc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/people-r-people_2025-05-05\n",
    "!onnx2tf -i people-r-people.onnx \\\n",
    "    -oiqt \\\n",
    "    -o people-r-people-tf  \\\n",
    "    -cind \"frame\" \"sample_images/calibdata.npy\" \"[[[[0,0,0]]]]\" \"[[[[1,1,1]]]]\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a570117",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b812eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from ai_edge_litert.interpreter import Interpreter, load_delegate  # AI Edge Lite / TFLite Runtime\n",
    "# from tensorflow.lite import Interpreter  # Uncomment if using full TensorFlow instead\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from importlib import reload\n",
    "import functions.parse_darknet_cfg as parse_darknet_cfg_mod\n",
    "reload(parse_darknet_cfg_mod)\n",
    "import functions.preprocess_image as preprocess_image_mod\n",
    "reload(preprocess_image_mod)\n",
    "import functions.postprocess_output as postprocess_output_mod\n",
    "reload(postprocess_output_mod)\n",
    "import functions.visualize_detections as visualize_detections_mod\n",
    "reload(visualize_detections_mod)\n",
    "\n",
    "\n",
    "def load_interpreter(model_path, use_edgetpu=False):\n",
    "    \"\"\"\n",
    "    Load a TensorFlow Lite or EdgeTPU interpreter.\n",
    "    \"\"\"\n",
    "    if use_edgetpu:\n",
    "        delegates = [load_delegate('libedgetpu.so.1')]\n",
    "        interpreter = Interpreter(model_path=model_path, experimental_delegates=delegates)\n",
    "    else:\n",
    "        interpreter = Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "\n",
    "def run_inference_tflite(model_path, image, yolo_layers, input_width, input_height, class_names, use_edgetpu=False):\n",
    "    \"\"\"\n",
    "    Run inference on a single image using a TFLite/AI Edge Lite model.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    interpreter = load_interpreter(model_path, use_edgetpu)\n",
    "\n",
    "    # Get input & output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape']\n",
    "\n",
    "    # Preprocess image\n",
    "    input_data, scale, padding = preprocess_image_mod.preprocess_image(input_width, input_height, image)\n",
    "\n",
    "\n",
    "    # NCHW -> NCHW\n",
    "    input_data = np.transpose(input_data, (0, 2, 3, 1))\n",
    "\n",
    "    if input_details[0][\"dtype\"] != np.float32:\n",
    "        quant_scale, quant_zero = input_details[0]['quantization']\n",
    "\n",
    "        input_data = (input_data.astype(np.float32) / quant_scale + quant_zero).round().astype(input_details[0][\"dtype\"])\n",
    "\n",
    "        # TODO: Detect output reversion instead of just assuming it for int8\n",
    "        yolo_layers = list(reversed(yolo_layers))\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    start_time = time.time()\n",
    "    interpreter.invoke()\n",
    "    inference_time = (time.time() - start_time) * 1000  # ms\n",
    "\n",
    "    def dequant(tensor, details):\n",
    "        if details[\"dtype\"] != np.float32:\n",
    "            scale_out, zero_point_out = details['quantization']\n",
    "\n",
    "            return (tensor.astype(np.float32) - zero_point_out) * scale_out\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "    # Gather outputs (NHWC -> NCHW)\n",
    "    outputs = [np.transpose(dequant(interpreter.get_tensor(o['index']), o), (0, 3, 1, 2)) for o in output_details]\n",
    "\n",
    "\n",
    "    # Postprocess    \n",
    "    print(\"Postprocessing...\")\n",
    "    detections = postprocess_output_mod.postprocess_output(\n",
    "        outputs,\n",
    "        yolo_layers_cfg=yolo_layers,\n",
    "        input_size=(input_width, input_height),\n",
    "        conf_threshold=CONF_THRESHOLD,\n",
    "        iou_threshold=IOU_THRESHOLD,\n",
    "        scale=scale,\n",
    "        padding=padding,\n",
    "        class_names=class_names,\n",
    "    )\n",
    "\n",
    "    print(f\"Inference completed in {inference_time:.2f} ms\")\n",
    "    return detections\n",
    "\n",
    "\n",
    "DARKNET_MODEL_BASE_NAME=\"people-r-people_2025-05-05/people-r-people\"\n",
    "NAMES_FILE=f\"{DARKNET_MODEL_BASE_NAME}.names\"\n",
    "CFG_FILE=f\"{DARKNET_MODEL_BASE_NAME}.cfg\"\n",
    "# TF_MODEL=\"people-r-people_2025-05-05/people-r-people-tf/people-r-people_float32.tflite\"\n",
    "TF_MODEL=\"people-r-people_2025-05-05/people-r-people-tf/people-r-people_full_integer_quant.tflite\"\n",
    "CONF_THRESHOLD=0.25\n",
    "IOU_THRESHOLD=0.45\n",
    "IMAGES_PATH=\"people-r-people_2025-05-05/sample_images\"\n",
    "\n",
    "# Load class names\n",
    "with open(NAMES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    class_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"✅ Loaded {num_classes} classes\")\n",
    "\n",
    "# Load darknet cfg\n",
    "net, layers = parse_darknet_cfg_mod.parse_darknet_cfg(CFG_FILE)\n",
    "input_width=net[\"width\"]\n",
    "input_height=net[\"height\"]\n",
    "yolo_layers = [layer for layer in layers if layer['type'] == 'yolo']\n",
    "\n",
    "for image_name in [img for img in os.listdir(IMAGES_PATH) if img.endswith('.jpg')]:\n",
    "    image_path = f\"{IMAGES_PATH}/{image_name}\"\n",
    "\n",
    "    print(f\"Processing image {image_path}\")\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    print(\"Running inference...\")\n",
    "    detections = run_inference_tflite(TF_MODEL, image=image, \n",
    "                                      yolo_layers=yolo_layers,\n",
    "                                        input_width=input_width,\n",
    "                                        input_height=input_height,\n",
    "        class_names=class_names,\n",
    "                                      \n",
    "                                      use_edgetpu=False)\n",
    "\n",
    "\n",
    "    print(\"Visualize...\")\n",
    "    visualized = visualize_detections_mod.visualize_detections(\n",
    "        image=image,\n",
    "        detections=detections[0],\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(visualized, cv2.COLOR_BGR2RGB),cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
